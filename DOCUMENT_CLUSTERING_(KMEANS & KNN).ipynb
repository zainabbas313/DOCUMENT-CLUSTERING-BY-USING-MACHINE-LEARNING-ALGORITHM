{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8860b3a6-6ea5-434a-8e35-a33e7bd88ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "import csv\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "51abd3bc-03f2-488e-8258-d4675309c1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Zain\n",
      "[nltk_data]     Abbas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Zain\n",
      "[nltk_data]     Abbas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a5f6a-7f3e-47f3-a6a8-949e570ba763",
   "metadata": {},
   "source": [
    "# Text Data Processing Toolkit\r\n",
    "\r\n",
    "This toolkit provides functions to process text data extracted from research papers. It includes functions for tokenization, cleaning, and segmentation of text.\r\n",
    "\r\n",
    "## Functions\r\n",
    "\r\n",
    "### 1. `remove_stopwords(tokens)`\r\n",
    "\r\n",
    "- **Purpose**: Removes stopwords and punctuation from a list of tokens.\r\n",
    "- **Input**: \r\n",
    "  - `tokens`: List of tokens to be processed.\r\n",
    "- **Output**:\r\n",
    "  - `filtered_tokens`: List of tokens with stopwords and punctuation removed.\r\n",
    "\r\n",
    "### 2. `extract_and_index_data(directory)`\r\n",
    "\r\n",
    "- **Purpose**: Extracts and indexes data from text files in a directory.\r\n",
    "- **Input**:\r\n",
    "  - `directory`: Path to the directory containing text files.\r\n",
    "- **Output**:\r\n",
    "  - Categorized tokens:\r\n",
    "    - `token1`: Tokens of length 3-14 characters.\r\n",
    "    - `token_of_len_2`: Tokens of length 2 characters.\r\n",
    "    - `token_as_sentence`: Tokens longer than 14 characters.\r\n",
    "    - `token_have_hyphen`: Tokens containing hyphens.\r\n",
    "    - `token_have_punctuation`: Tokens containing punctuation.\r\n",
    "    - `all_token`: All other tokens.\r\n",
    "    - `all_number`: Tokens consisting only of numbers.\r\n",
    "\r\n",
    "### 3. `cleaning_pipeline(token)`\r\n",
    "\r\n",
    "- **Purpose**: Further cleans tokenized data by splitting tokens with multiple words, removing non-alphabetic characters, and filtering tokens based on their length and presence in the English dictionary.\r\n",
    "- **Input**:\r\n",
    "  - `token`: List of tokens to be cleaned.\r\n",
    "- **Output**:\r\n",
    "  - `tokens_no_further_processing_required`: Cleaned tokens not requiring further processing.\r\n",
    "  - `clean_token`: Cleaned tokens after initial processing.\r\n",
    "  - `token_contain_only_numbers`: Tokens containing only numbers.\r\n",
    "\r\n",
    "### 4. `token_seperator(token)`\r\n",
    "\r\n",
    "- **Purpose**: Separates concatenated tokens using dynamic programming and further cleans the tokens.\r\n",
    "- **Input**:\r\n",
    "  - `token`: List of tokens to be separated.\r\n",
    "- **Output**:\r\n",
    "  - `tokens_no_further_processing_required`: Cleaned tokens not requiring further processing.\r\n",
    "  - `clean_token`: Cleaned tokens after initial processing.\r\n",
    "  - `token_contain_only_numbers`: Tokens containing only numbers.\r\n",
    "\r\n",
    "### 5. `remove_conjunctions_from_sets(input_list)`\r\n",
    "\r\n",
    "- **Purpose**: Removes common conjunctions from a list of tokens.\r\n",
    "- **Input**:\r\n",
    "  - `input_list`: List of tokens to be processed.\r\n",
    "- **Output**:\r\n",
    "  - `filtered_list`: List of tokens with common conjunctions removed.\r\n",
    "\r\n",
    "### 6. `word_segment(sentences)`\r\n",
    "\r\n",
    "- **Purpose**: Segments words from sentences, handling cases of concatenated words and filtering out stopwords and short words.\r\n",
    "- **Input**:\r\n",
    "  - `sentences`: List of sentences to be processed.\r\n",
    "- **Output**:\r\n",
    "  - Segmented words.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "10beb854-fa79-41a0-96ab-24423e800582",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set([\"a\", \"is\", \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\", \"we\", \"do\"])\n",
    "punctuation = set(['!', '\\\\', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '—'])\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "#remove_stopwords, simply remove the stopwords and the punctuatated tokens\n",
    "def remove_stopwords(tokens):    \n",
    "    punctuation = set(string.punctuation)\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
    "    return filtered_tokens\n",
    "\n",
    "def extract_and_index_data(directory):\n",
    "    \n",
    "    token1 = []\n",
    "    token_of_len_2 = []\n",
    "    token_as_sentence = []\n",
    "    token_have_hyphen = []\n",
    "    token_have_punctuation = []\n",
    "    all_token = []\n",
    "    all_number = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "  \n",
    "        if os.path.isfile(os.path.join(directory, filename)):\n",
    "            with open(os.path.join(directory, filename), 'r') as text:\n",
    "                data = text.read()\n",
    "\n",
    "            file_tokens = word_tokenize(data)\n",
    "            normalized = remove_stopwords(file_tokens)\n",
    "\n",
    "            for token in normalized:\n",
    "                if 3 <= len(token) <= 14 and all(char not in string.punctuation for char in token) and not token.isdigit():\n",
    "                    token1.append((token.lower(), filename))\n",
    "                elif len(token) == 2 and not token.isdigit():\n",
    "                    token_of_len_2.append((token.lower(), filename))\n",
    "                elif len(token) > 14 and '-' not in token and not token.isdigit():\n",
    "                    token_as_sentence.append((token.lower(), filename))\n",
    "                elif '-' in token:\n",
    "                    token_have_hyphen.append((token.lower(), filename))\n",
    "                elif any(char in string.punctuation for char in token) and '-' not in token and len(token) <= 14 and not token.isdigit():\n",
    "                    token_have_punctuation.append((token.lower(), filename))\n",
    "                elif token.isdigit():\n",
    "                    all_number.append((token.lower(), filename))\n",
    "                else:\n",
    "                    all_token.append((token.lower(), filename))\n",
    "\n",
    "    return (token1, token_of_len_2, token_as_sentence, token_have_hyphen, token_have_punctuation, all_token, all_number)\n",
    "\n",
    "\n",
    "directory_path = r'D:\\SEMESTER 06\\INFORMATION RETRIEVAL\\ASSIGNMENT II\\ResearchPapers'\n",
    "#function invoking\n",
    "tokens= extract_and_index_data(directory_path)\n",
    "\n",
    "token1, token_of_len_2, token_as_sentence, token_have_hyphen, token_have_punctuation, all_token, all_number = tokens\n",
    "\n",
    "#########################   \n",
    "\n",
    "def cleaning_pipeline(token):\n",
    "    clean_token = []\n",
    "    token_split = []\n",
    "    new_token = []\n",
    "    tokens_no_further_processing_required = []\n",
    "    token_contain_only_numbers = []\n",
    "    for x, y in token:\n",
    "        if len(x) >= 4 and not x.isdigit() and any(c.isalpha() for c in x):\n",
    "            new_x = ''.join(' ' if char in punctuation else char for char in x)\n",
    "            if ' ' in new_x:\n",
    "                token_split = new_x.split(' ')\n",
    "                for word in token_split:\n",
    "                    if len(word) >= 4:\n",
    "                        new_token.append((word, y))\n",
    "            else:\n",
    "                if len(new_x) >= 4:\n",
    "                    new_token.append((new_x, y))\n",
    "        elif x.isdigit():\n",
    "            token_contain_only_numbers.append((x, y))\n",
    "    \n",
    "    for x,y in new_token:\n",
    "        if x.isdigit():\n",
    "            token_contain_only_numbers.append((x,y))\n",
    "\n",
    "    \n",
    "    for x, y in new_token:\n",
    "        if any(char.isdigit() for char in x):\n",
    "            cleaned_x = ''.join(char if char != '—' else '' for char in x) and ''.join(char for char in x if not char.isdigit())\n",
    "            split_tokens = nltk.word_tokenize(cleaned_x)\n",
    "            for word in split_tokens:\n",
    "                if len(word) >= 4:\n",
    "                    clean_token.append((word, y))\n",
    "        else:\n",
    "            clean_token.append((x, y))\n",
    "    tokens_no_further_processing_required = [(x,y) for x,y in clean_token if x in english_words]\n",
    "    clean_token = [(x, y) for x, y in clean_token if x not in english_words]\n",
    "    return tokens_no_further_processing_required,clean_token,token_contain_only_numbers\n",
    "\n",
    "#########################################\n",
    "\n",
    "def token_seperator(token):\n",
    "    words = []\n",
    "    for x in english_words:\n",
    "        words.append(x)\n",
    "\n",
    "    for x,y in token1:\n",
    "        words.append(x)\n",
    "\n",
    "\n",
    "    wordcost = {k: log((i + 1) * log(len(words))) for i, k in enumerate(words)}\n",
    "    maxword = max(len(x) for x in words)\n",
    "    # return infer_spaces(token,maxword,wordcost)\n",
    "    clean_term = []\n",
    "    for i in range(len(token)):\n",
    "        clean_term.append((infer_spaces(token[i][0].lower(),maxword,wordcost),token[i][1]))\n",
    "    return cleaning_pipeline(clean_term)\n",
    "    \n",
    "\n",
    "def infer_spaces(s,maxword,wordcost):\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i - maxword):i]))\n",
    "        return min((c + wordcost.get(s[i - k - 1:i], 9e999), k + 1) for k, c in candidates)\n",
    "    \n",
    "    cost = [0]\n",
    "    for i in range(1, len(s) + 1):\n",
    "        c, k = best_match(i)\n",
    "        cost.append(c)\n",
    "    \n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i > 0:\n",
    "        c, k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i - k:i])\n",
    "        i -= k\n",
    "    \n",
    "    return \" \".join(reversed(out))\n",
    "    \n",
    "#######################################\n",
    "\n",
    "\n",
    "def remove_conjunctions_from_sets(input_list):\n",
    "    conjunctions = ['this', 'that','of', 'and', 'or', 'but', 'for', 'nor', 'so', 'yet', 'to', 'with', 'in', 'on', 'at', 'by', 'is', \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\"]\n",
    "    filtered_list = []\n",
    "    for s, x in input_list:  # Unpack the tuple\n",
    "        # Check from the beginning\n",
    "        for i in range(len(s)):\n",
    "            first_word = s[:i].lower()\n",
    "            if first_word in conjunctions:\n",
    "                s = s[i:]\n",
    "                break\n",
    "        # Check from the end\n",
    "        for i in range(len(s), 0, -1):\n",
    "            last_word = s[i:].lower()\n",
    "            if last_word in conjunctions:\n",
    "                s = s[:i]\n",
    "                break\n",
    "        filtered_list.append((s, x))  # Append the modified tuple\n",
    "    return filtered_list\n",
    "\n",
    "\n",
    "def word_segment(sentences):\n",
    "    len_3_word = []\n",
    "    segmented_sentences = []\n",
    "    term = []\n",
    "    for sentence, info in sentences:\n",
    "        words = []\n",
    "        start = 0\n",
    "        while start < len(sentence):\n",
    "            found = False\n",
    "            for end in range(len(sentence), start, -1):\n",
    "                word = sentence[start:end]\n",
    "                if word.lower() in english_words:\n",
    "                    if word.lower().startswith(\"the\"):\n",
    "                        word = word[2:]\n",
    "                    if len(word) >= 4:\n",
    "                        if word.lower() not in {\"a\", \"an\", \"the\"} or start != 0:\n",
    "                            words.append(word)\n",
    "                        start = end\n",
    "                        found = True\n",
    "                    else:\n",
    "                        len_3_word.append((word, info))\n",
    "                    break\n",
    "            if not found:\n",
    "                start += 1\n",
    "        segmented_sentences.append((words, info))\n",
    "\n",
    "    term = [(wd.lower(), info) for wd, info in len_3_word if len(wd) >= 3 and wordnet.synsets(word, pos=wordnet.NOUN) and word.lower() not in stop_words and word not in punctuation]\n",
    "    for x, y in segmented_sentences:\n",
    "        for word in x:\n",
    "            if word not in stop_words and word in english_words:\n",
    "                term.append((word, y))\n",
    "    return term\n",
    "\n",
    "####################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0f1ea-e18c-4128-9fe5-b92607cadf29",
   "metadata": {},
   "source": [
    "# Additional Token Processing\n",
    "\n",
    "## Hyphenated Terms and Links\n",
    "\n",
    "Hyphenated terms and links are separated and filtered from the main token list.\n",
    "\n",
    "- **Hyphenated Terms**:\n",
    "  - Terms containing a single hyphen are extracted and checked for length and hyphen position.\n",
    "  - The extracted terms are added to the `hyphen_term` list.\n",
    "  \n",
    "- **Links**:\n",
    "  - Tokens starting with common link prefixes like 'http://', 'www', or 'org' are filtered and added to the `token_link` list.\n",
    "\n",
    "## Further Token Filtering\n",
    "\n",
    "Additional token filtering is performed to refine the token lists.\n",
    "\n",
    "- **Tokens with Hyphens**:\n",
    "  - Tokens separated in the previous step are removed from the main token list.\n",
    "\n",
    "- **Tokens Considered as Sentences**:\n",
    "  - Tokens resembling sentences, often starting with link prefixes, are filtered out.\n",
    "\n",
    "## Vocabulary Check\n",
    "\n",
    "Tokens are checked against an English vocabulary to filter out non-English words.\n",
    "\n",
    "- **Existing in Vocabulary**:\n",
    "  - Tokens found in the English vocabulary are separated into a new list (`token_exist_in_vocab`), while others remain in the `token_as_sentence` list.\n",
    "\n",
    "## Cleaning and Processing\n",
    "\n",
    "The remaining tokens undergo further cleaning and processing.\n",
    "\n",
    "- **Cleaning Pipeline**:\n",
    "  - The `cleaning_pipeline` function is applied to tokens categorized as sentences and tokens with punctuation, resulting in cleaned tokens.\n",
    "\n",
    "- **Token Separation**:\n",
    "  - The `token_seperator` function is used to separate tokens that might be concatenated words.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Finally, stemming is applied to the tokens, reducing them to their root forms.\n",
    "\n",
    "- **Stemming**:\n",
    "  - Each token is stemmed using a stemming algorithm (e.g., Porter Stemmer), and the stemmed tokens are collected in the `stem_terms` list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "864b2550-9de3-4d31-b3dc-3af2ed5da886",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyphen_term = []\n",
    "token_exist_in_vocab = []\n",
    "\n",
    "token_link = [(x, y) for x, y in token_have_hyphen if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "token_have_hyphen = [(x, y) for x, y in token_have_hyphen if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "hyphen_term = [(x, y) for x, y in token_have_hyphen if x.count('-') == 1 and '-' not in [x[0], x[-1]] and 4 <= len(x.split('-')[0]) + len(x.split('-')[1]) <= 20]\n",
    "token_have_hyphen = [(x, y) for x, y in token_have_hyphen if (x, y) not in hyphen_term]\n",
    "\n",
    "token_link = [(x, y) for x, y in token_as_sentence if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "token_as_sentence = [(x, y) for x, y in token_as_sentence if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "\n",
    "token_exist_in_vocab = [(x,y) for x,y in token_as_sentence if x in english_words]\n",
    "token_as_sentence = [(x, y) for x, y in token_as_sentence if x not in english_words]\n",
    "\n",
    "token1 += token_exist_in_vocab\n",
    "\n",
    "tokens_no_further_processing_required, clean_token, token_contain_only_numbers = cleaning_pipeline(token_as_sentence)\n",
    "all_number += token_contain_only_numbers\n",
    "token1 += tokens_no_further_processing_required\n",
    "\n",
    "\n",
    "tokens_no_further_processing_required1, clean_token1, token_contain_only_numbers1 = cleaning_pipeline(token_have_punctuation)\n",
    "all_number += token_contain_only_numbers1\n",
    "token1 += tokens_no_further_processing_required1\n",
    "clean_token += clean_token1\n",
    "\n",
    "tokens_no_further_processing_required3, clean_token3, token_contain_only_numbers3 = cleaning_pipeline(token_have_hyphen)\n",
    "all_number += token_contain_only_numbers3\n",
    "token1 += tokens_no_further_processing_required3\n",
    "clean_token += clean_token3\n",
    "\n",
    "tokens_no_further_processing_required2, clean_token2,token_contain_only_numbers2 = token_seperator(clean_token)\n",
    "token1 += tokens_no_further_processing_required2\n",
    "\n",
    "sets_list = remove_conjunctions_from_sets(clean_token2)\n",
    "token_sentences = word_segment(sets_list)\n",
    "\n",
    "term = []\n",
    "\n",
    "term = token_link + token1 + all_number + token_sentences + hyphen_term\n",
    "\n",
    "stem_terms = []\n",
    "for x, y in term:\n",
    "    stemmed_x = porter.stem(x)\n",
    "    stem_terms.append((stemmed_x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "96b2d004-225a-4c1e-b35d-0a1e3ee98365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "[1, 2, 3, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "doc_names = ['1.txt', '2.txt', '3.txt', '7.txt', '8.txt', '9.txt', '11.txt', '12.txt', '13.txt', '14.txt', '15.txt', '16.txt', '17.txt', '18.txt', '21.txt', '22.txt', '23.txt', '24.txt', '25.txt', '26.txt']\n",
    "print(len(doc_names))\n",
    "doc = [int(re.search(r'\\d+', doc_name).group()) for doc_name in doc_names]\n",
    "\n",
    "doc.sort()\n",
    "print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d4fdc3f2-b7aa-438d-9b05-361fec63066c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {file: [] for file in doc}\n",
    "for term, file in stem_terms:\n",
    "    file_idx = int(file[0].split('.')[0])\n",
    "    inverted_index[file_idx].append(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "25e1a5cb-1e85-4758-8e9b-1442ea93bbaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[//doi.org/10.1002/widm.1391, //wires.onlineli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[//featureselection.asu.edu/, //doi.org/10.114...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[//portal.acm.org/citation.cfm, //doi.org/10.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>[//www.fairmlbook.org, //kilthub.cmu.edu/artic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>[//doi.org/10.36628/ijhf.2023.0050, //doi.org/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   File                                              Terms\n",
       "0     1  [//doi.org/10.1002/widm.1391, //wires.onlineli...\n",
       "1     2  [//featureselection.asu.edu/, //doi.org/10.114...\n",
       "2     3  [//portal.acm.org/citation.cfm, //doi.org/10.1...\n",
       "3     7  [//www.fairmlbook.org, //kilthub.cmu.edu/artic...\n",
       "4     8  [//doi.org/10.36628/ijhf.2023.0050, //doi.org/..."
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)  \n",
    "pd.set_option('display.max_rows', None)\n",
    "df = pd.DataFrame(inverted_index.items(), columns=['File', 'Terms'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8733e3-4c78-4a7a-ad3c-f54d74383f24",
   "metadata": {},
   "source": [
    "# BOW: BAG OF WORDS\n",
    "## Bag-of-Words Representation\r\n",
    "\r\n",
    "This section describes the creation of a Bag-of-Words (BoW) representation from the processed token data.\r\n",
    "\r\n",
    "## Construction of BoW DataFrame\r\n",
    "\r\n",
    "A BoW DataFrame (`bow_df`) is constructed to represent the occurrence of terms across documents.\r\n",
    "\r\n",
    "- **DataFrame Structure**:\r\n",
    "  - Each row represents a document.\r\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\r\n",
    "\r\n",
    "## Generating BoW Data\r\n",
    "\r\n",
    "The BoW DataFrame is populated with term frequencies for each document.\r\n",
    "\r\n",
    "- **Term Frequencies**:\r\n",
    "  - For each document (`file`), a Counter object is created to count the occurrences of terms.\r\n",
    "  - The frequency of each term in the document is recorded in the corresponding column of the DataFrame.\r\n",
    "\r\n",
    "## Filling Missing Values\r\n",
    "\r\n",
    "Any missing values (NaN) in the DataFrame are filled with zeros to represent terms that do not occur in certain documents.\r\n",
    "\r\n",
    "- **Handling Missing Values**:\r\n",
    "  - Missing values in the DataFrame are replaced with zeros using the `fillna` method, ensuring consistent representation across all documents.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "c6665321-1ecf-4d56-bee4-9a3b13b1a31e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_terms = sorted(set(x for x,y in stem_terms))\n",
    "bow_df = pd.DataFrame(columns=all_terms, index=doc)\n",
    "\n",
    "for file, terms in inverted_index.items():\n",
    "    term_counts = Counter(terms)\n",
    "    bow_df.loc[file] = [term_counts[term] for term in all_terms]\n",
    "\n",
    "bow_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4351a530-00dd-4dfc-a158-c5ee3350152a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71331005</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717–727</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71:2668-79</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–101</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–104</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–80</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–82</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–83</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1  2  3  7  8  9  11 12 13 14 15 16 17 18 21 22 23 24 25 26\n",
       "71331005    0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "717–727     1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "71:2668-79  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "71–101      0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "71–104      1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "71–80       1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "71–82       1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "71–83       1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "72          2  3  0  8  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
       "720         1  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = bow_df.T\n",
    "bow.iloc[10000:10010]\n",
    "bow.iloc[2190:2200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a773fd8-bb45-41c2-85dd-27edb7fcce3a",
   "metadata": {},
   "source": [
    "# TF: TERM FREQUENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ea4d6f-3b70-4336-958b-1da05d7ea11e",
   "metadata": {},
   "source": [
    "## Construction of TF Matrix\n",
    "\n",
    "A TF matrix (`tf_matrix`) is constructed to represent the term frequencies normalized by document length.\n",
    "\n",
    "- **Matrix Structure**:\n",
    "  - Each row represents a document.\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\n",
    "\n",
    "## Generating TF Data\n",
    "\n",
    "The TF matrix is populated with term frequencies normalized by document length.\n",
    "\n",
    "- **Normalization Formula**:\n",
    "  - Term frequencies are transformed using the formula: \\( 1 + \\log_{10}(tf_{ij} + 1) \\), where \\( tf_{ij} \\) is the term frequency of term \\( j \\) in document \\( i \\).\n",
    "\n",
    "- **Normalization Process**:\n",
    "  - For each term in each document, the term frequency is normalized using the logarithmic transformation.\n",
    "  - The normalized term frequencies are recorded in the corresponding cells of the TF matrix.\n",
    "\n",
    "## Filling Missing Values\n",
    "\n",
    "Any missing values (NaN) in the TF matrix are filled with zeros.\n",
    "\n",
    "- **Handling Missing Values**:\n",
    "  - Missing values in the TF matrix are replaced with zeros using the `fillna` method, ensuring consistent representation of term frequencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "09b01061-1210-438b-808e-2a4a6b5ed833",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf_matrix = pd.DataFrame(columns=all_terms, index=doc)\n",
    "tf_matrix = bow.copy()\n",
    "for i in range(len(all_terms)):\n",
    "    for j in range(20):\n",
    "        tf_matrix.iloc[i,j] = 1 + np.log10(bow.iloc[i,j] + 1)\n",
    "\n",
    "tf_matrix.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c553c-c006-4403-ae99-cefa9194a112",
   "metadata": {},
   "source": [
    "# Explanation of Term Frequency Calculation\n",
    "\n",
    "The formula `1 + np.log10(bow.iloc[i,j] + 1)` is used to calculate the Term Frequency (TF) for each term in the Bag-of-Words (BoW) representation.\n",
    "\n",
    "## TF Calculation Formula\n",
    "\n",
    "The TF value for a term in a document is determined using the following formula:\n",
    "\n",
    "\\[ \\text{TF}_{ij} = 1 + \\log_{10}(\\text{tf}_{ij} + 1) \\]\n",
    "\n",
    "- \\( \\text{TF}_{ij} \\): Term Frequency of term \\( j \\) in document \\( i \\)\n",
    "- \\( \\text{tf}_{ij} \\): Raw frequency of term \\( j \\) in document \\( i \\)\n",
    "\n",
    "## Explanation of Formula\n",
    "\n",
    "1. **Addition of 1**: \n",
    "   - Adding 1 to the raw term frequency ensures that terms with zero occurrences are assigned a non-zero TF value.\n",
    "\n",
    "2. **Logarithmic Transformation**:\n",
    "   - The raw term frequency is transformed using a logarithmic function (base 10).\n",
    "   - Logarithmic scaling compresses the range of term frequencies, reducing the influence of very high frequencies.\n",
    "\n",
    "3. **Normalization**:\n",
    "   - By adding 1 before taking the logarithm, the TF values are normalized, preventing extremely high frequencies from dominating the TF calculation.\n",
    "   - Normalization helps balance the importance of terms within documents, especially for documents with varying lengths.\n",
    "\n",
    "## Impact on Term Weight\n",
    "\n",
    "- **Increase in Weight**:\n",
    "  - The formula increases the weight assigned to terms based on their frequency in documents.\n",
    "  - Terms occurring more frequently in a document are assigned higher TF values, reflecting their relative importance within the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "88a67f01-4112-4608-afe6-c75899eac51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71331005</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717–727</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71:2668-79</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–101</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–104</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–80</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–82</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71–83</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.60206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.954243</td>\n",
       "      <td>1.60206</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1         2    3         7        8    9    11   12   13  \\\n",
       "71331005         1.0   1.30103  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "717–727      1.30103       1.0  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "71:2668-79       1.0       1.0  1.0       1.0  1.30103  1.0  1.0  1.0  1.0   \n",
       "71–101           1.0       1.0  1.0   1.30103      1.0  1.0  1.0  1.0  1.0   \n",
       "71–104       1.30103       1.0  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "71–80        1.30103       1.0  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "71–82        1.30103       1.0  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "71–83        1.30103       1.0  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "72          1.477121   1.60206  1.0  1.954243  1.60206  1.0  1.0  1.0  1.0   \n",
       "720          1.30103  1.477121  1.0       1.0      1.0  1.0  1.0  1.0  1.0   \n",
       "\n",
       "             14   15   16   17   18   21   22   23   24   25   26  \n",
       "71331005    1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "717–727     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "71:2668-79  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "71–101      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "71–104      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "71–80       1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "71–82       1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "71–83       1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "72          1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "720         1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.head()\n",
    "tf_matrix.iloc[2190:2200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "c2d1c294-62d5-4604-980a-cd9cb2f3ee8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mich</th>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>1.778151</td>\n",
       "      <td>1.69897</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michaelcollin</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michailidi</th>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michal</th>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michalski</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michel</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michi</th>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michigan</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.845098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.477121</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1         2        3         7    8    9    11   12   13  \\\n",
       "mich            1.30103       1.0      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michael        1.778151   1.69897      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michaelcollin       1.0   1.30103      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michailidi     1.477121       1.0      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michal         1.477121       1.0      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michalski           1.0       1.0      1.0  1.477121  1.0  1.0  1.0  1.0  1.0   \n",
       "michel              1.0   1.30103      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michi          1.477121       1.0      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "michigan            1.0  1.845098      1.0       1.0  1.0  1.0  1.0  1.0  1.0   \n",
       "micro               1.0  1.477121  1.30103   1.30103  1.0  1.0  1.0  1.0  1.0   \n",
       "\n",
       "                14   15   16   17   18   21   22   23   24   25   26  \n",
       "mich           1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michael        1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michaelcollin  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michailidi     1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michal         1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michalski      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michel         1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michi          1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "michigan       1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "micro          1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = tf_matrix.T\n",
    "tf_matrix.iloc[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605e4fb-879f-4ebd-ae68-e5387601f6c9",
   "metadata": {},
   "source": [
    "# IDF: INVERSE DOCUMENT FREQUENCY\n",
    "\n",
    "# Inverse Document Frequency (IDF) Matrix\r\n",
    "\r\n",
    "This section describes the generation of the Inverse Document Frequency (IDF) matrix from the Bag-of-Words representation.\r\n",
    "\r\n",
    "## Construction of IDF Matrix\r\n",
    "\r\n",
    "An IDF matrix (`idf_matrix`) is constructed to represent the inverse document frequencies of terms.\r\n",
    "\r\n",
    "- **Matrix Structure**:\r\n",
    "  - The IDF matrix has a single row representing IDF values for all terms.\r\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\r\n",
    "\r\n",
    "## Generating IDF Data\r\n",
    "\r\n",
    "The IDF matrix is populated with IDF values calculated based on document frequencies.\r\n",
    "\r\n",
    "- **IDF Calculation**:\r\n",
    "  - IDF values are calculated using the formula: \\( \\log_{10} \\left( \\frac{N}{df_t} \\right) \\), where \\( N \\) is the total number of documents and \\( df_t \\) is the number of documents containing term \\( t \\).\r\n",
    "\r\n",
    "- **IDF Calculation Process**:\r\n",
    "  - For each term in the document, the number of documents containing the term (\\( df_t \\)) is counted.\r\n",
    "  - IDF values are calculated based on the total number of documents and the document frequency of each term.\r\n",
    "  - The calculated IDF values are recorded in the corresponding cells of the IDF matrix.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9d41a1b6-34de-413e-8935-50a10b326588",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_freq = (bow_df > 0).sum(axis=0)\n",
    "\n",
    "idf_values = np.log10(len(doc) / doc_freq)\n",
    "\n",
    "idf_matrix = pd.DataFrame([idf_values], columns=all_terms, index=['idf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "74117c62-8e64-41e6-a1fa-c831ad2972bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>559–583</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55–60</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55–79</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.69897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561–563</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563–572</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565–566</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567–568</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567–570</th>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             idf\n",
       "559–583  1.30103\n",
       "55–60    1.30103\n",
       "55–79    1.30103\n",
       "56       0.69897\n",
       "561–563  1.30103\n",
       "563      1.30103\n",
       "563–572  1.30103\n",
       "565–566  1.30103\n",
       "567–568  1.30103\n",
       "567–570  1.30103"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = idf_matrix.T\n",
    "idf.iloc[2000:2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "517a5ac1-d162-4fb3-8655-4e8fc365bb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf.to_csv('idf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9269ce18-9ee1-4821-a260-90dc9c6ab2ee",
   "metadata": {},
   "source": [
    "# COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c85162-a9c4-451d-987b-df4e620a8800",
   "metadata": {},
   "source": [
    "## WEIGHT MATRIX\n",
    "# Calculation of Weighted Term Frequencies\r\n",
    "\r\n",
    "This section describes the calculation of weighted term frequencies using TF-IDF values.\r\n",
    "\r\n",
    "## Construction of Weighted Matrix\r\n",
    "\r\n",
    "A weighted matrix (`weight`) is constructed to represent the weighted term frequencies based on TF-IDF values.\r\n",
    "\r\n",
    "- **Matrix Structure**:\r\n",
    "  - Each row represents a document.\r\n",
    "  - Each column represents a unique stemmed term extracted from the documents.\r\n",
    "\r\n",
    "## Generating Weighted Data\r\n",
    "\r\n",
    "The weighted matrix is populated with weighted term frequencies calculated using TF-IDF values.\r\n",
    "\r\n",
    "- **Weighted Calculation**:\r\n",
    "  - Weighted term frequencies are calculated by multiplying TF values with IDF values for each term in each document.\r\n",
    "\r\n",
    "- **Weighted Calculation Process**:\r\n",
    "  - For each term in each document, the TF value is multiplied by the corresponding IDF value to calculate the weighted term frequency.\r\n",
    "  - The calculated weighted term frequencies are recorded in the corresponding cells of the weighted matrix.\r\n",
    "\r\n",
    "## Cosine Similarity\r\n",
    "\r\n",
    "Once the weighted matrix is constructed, cosine similarity can be calculated to measure the similarity between documents.\r\n",
    "\r\n",
    "- **Cosine Similarity**:\r\n",
    "  - Cosine similarity measures the cosine of the angle between two vectors, representing document representations in a high-dimensional space.\r\n",
    "  - Higher cosine similarity values indicate greater similarity between documents, while values closer to 0 indicate dissimilarity.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "e0418fb3-6b20-426b-9b7d-ee0a2aee526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=all_terms, index=doc)\n",
    "\n",
    "for i in range(len(doc)):\n",
    "    for j in range(len(all_terms)):\n",
    "        weighted_value = tf.iloc[i, j] * idf.iloc[i, 0]\n",
    "        df.iloc[i, j] = weighted_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7db5a45e-4f0c-44d0-88f3-61b52a20c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('weight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e2948cae-e28d-477a-9300-0662a9179c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = pd.read_csv('weight.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fca63d4a-e537-4448-ad47-f9178e10488c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 16660)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c2320a73-f563-4989-b51e-13d3f341f281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mich</th>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michael</th>\n",
       "      <td>2.313428</td>\n",
       "      <td>2.210411</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michaelcollin</th>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michailidi</th>\n",
       "      <td>1.921779</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michal</th>\n",
       "      <td>1.921779</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michalski</th>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.921779</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michel</th>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michi</th>\n",
       "      <td>1.921779</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>michigan</th>\n",
       "      <td>1.301030</td>\n",
       "      <td>2.400528</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>micro</th>\n",
       "      <td>1.301030</td>\n",
       "      <td>1.921779</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.692679</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "      <td>1.30103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1         2         3         7        8        9   \\\n",
       "mich           1.692679  1.301030  1.301030  1.301030  1.30103  1.30103   \n",
       "michael        2.313428  2.210411  1.301030  1.301030  1.30103  1.30103   \n",
       "michaelcollin  1.301030  1.692679  1.301030  1.301030  1.30103  1.30103   \n",
       "michailidi     1.921779  1.301030  1.301030  1.301030  1.30103  1.30103   \n",
       "michal         1.921779  1.301030  1.301030  1.301030  1.30103  1.30103   \n",
       "michalski      1.301030  1.301030  1.301030  1.921779  1.30103  1.30103   \n",
       "michel         1.301030  1.692679  1.301030  1.301030  1.30103  1.30103   \n",
       "michi          1.921779  1.301030  1.301030  1.301030  1.30103  1.30103   \n",
       "michigan       1.301030  2.400528  1.301030  1.301030  1.30103  1.30103   \n",
       "micro          1.301030  1.921779  1.692679  1.692679  1.30103  1.30103   \n",
       "\n",
       "                    11       12       13       14       15       16       17  \\\n",
       "mich           1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michael        1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michaelcollin  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michailidi     1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michal         1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michalski      1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michel         1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michi          1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "michigan       1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "micro          1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103   \n",
       "\n",
       "                    18       21       22       23       24       25       26  \n",
       "mich           1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michael        1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michaelcollin  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michailidi     1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michal         1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michalski      1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michel         1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michi          1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "michigan       1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  \n",
       "micro          1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  1.30103  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = weight.T\n",
    "tf_idf[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddfe867-40cb-4781-b662-06041f71c0e0",
   "metadata": {},
   "source": [
    "# SUPERVISED CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b16a9-9c0a-4704-b70a-6d73602ea801",
   "metadata": {},
   "source": [
    "## CLASSIFICATION (TARGET VARIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "75e604a1-6674-447a-beed-a4781f633600",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = weight.assign(classes=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "912cf87c-a059-4cf9-a74b-ae5c60eee9bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 1, 1, 1, 1, 1],\n",
       "      dtype=int8)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {\n",
    "    \"Explainable Artificial Intelligence\": [1, 2, 3, 7],\n",
    "    \"Heart Failure\": [8, 9, 11],\n",
    "    \"Time Series Forecasting\": [12, 13, 14, 15, 16],\n",
    "    \"Transformer Model\": [17, 18, 21],\n",
    "    \"Feature Selection\": [22, 23, 24, 25, 26]\n",
    "}\n",
    "\n",
    "targets = []\n",
    "for doc_id in tf_idf.columns:\n",
    "    for class_name, doc_list in classes.items():\n",
    "        if doc_id in doc_list:\n",
    "            targets.append(class_name)\n",
    "            break\n",
    "\n",
    "target_labels = pd.Categorical(targets).codes\n",
    "\n",
    "target_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "aa8157ed-97d8-4587-9e6e-79e058cecfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, target_label in zip(weight.index, target_labels):\n",
    "    weight.at[index, 'classes'] = target_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "f3b7de23-52a2-4a09-ada6-9ce75a57be99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     0\n",
       "2     0\n",
       "3     0\n",
       "7     0\n",
       "8     2\n",
       "9     2\n",
       "11    2\n",
       "12    3\n",
       "13    3\n",
       "14    3\n",
       "15    3\n",
       "16    3\n",
       "17    4\n",
       "18    4\n",
       "21    4\n",
       "22    1\n",
       "23    1\n",
       "24    1\n",
       "25    1\n",
       "26    1\n",
       "Name: classes, dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight['classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb569d50-b7ff-4dff-8dc1-7a8514e4c5ab",
   "metadata": {},
   "source": [
    "## FEATURES SELECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e89c729-f210-41cc-9c03-518debc7e08b",
   "metadata": {},
   "source": [
    "## Feature Selection and Preprocessing Pipeline\n",
    "\n",
    "This README provides an in-depth understanding of the feature selection and preprocessing pipeline implemented in the code.\n",
    "\n",
    "### Code Overview:\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - The input data consists of features stored in the DataFrame `weight`. Features are stored in columns, with the target variable ('classes') separated.\n",
    "\n",
    "2. **Missing Value Imputation**:\n",
    "   - Missing values in the input features (`X`) are imputed using the mean strategy through `SimpleImputer`.\n",
    "\n",
    "3. **Feature Scaling**:\n",
    "   - The imputed features are then scaled using Min-Max scaling via `MinMaxScaler`. This ensures that all features are on the same scale, preventing any particular feature from dominating the analysis due to its larger magnitude.\n",
    "\n",
    "4. **Feature Selection**:\n",
    "   - Feature selection is performed using the chi-square test (`SelectKBest` with `chi2` score function). It selects the top `k=1000` features based on their chi-square scores, which measure the dependency between each feature and the target variable ('classes').\n",
    "   - The selected features are stored in `X_selected`.\n",
    "\n",
    "### Usage:\n",
    "\n",
    "1. **Input Data**:\n",
    "   - Ensure that the input data (`weight`) contains features along with the target variable ('classes').\n",
    "\n",
    "2. **Execution**:\n",
    "   - Execute the provided code to perform missing value imputation, feature scaling, and feature selection.\n",
    "   - The resulting selected features (`X_selected`) can be used for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d693b555-c753-4ba2-ad29-bf977edfde77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = weight.drop(['classes'], axis=1)\n",
    "y = weight['classes']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=1000)\n",
    "X_selected = selector.fit_transform(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a56a40-b7b0-44cc-9ca5-7758bc627c9c",
   "metadata": {},
   "source": [
    "## KNN CLASSIFICATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "fd334fc0-8794-426f-bdac-b1e207d2e9fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5 \n",
    "knn = KNeighborsClassifier(n_neighbors=k)\n",
    "knn.fit(X_selected, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a6f43b-1fc3-4bd6-8acb-c1470e0248e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc8c89f7-19f2-426b-8f38-39bd2290727e",
   "metadata": {},
   "source": [
    "# UNSPUERVISED CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3c713-c070-4d28-bc97-a94d95fc766b",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "## Clustering Evaluation with K-Means\r\n",
    "\r\n",
    "This code evaluates the performance of a K-Means clustering algorithm using three evaluation metrics: Purity, Silhouette Score, and Random Index.\r\n",
    "\r\n",
    "### Code Overview:\r\n",
    "\r\n",
    "1. **K-Means Clustering**:\r\n",
    "   - Utilizes the K-Means algorithm to cluster data into `k=5` clusters (`n_clusters=k`).\r\n",
    "\r\n",
    "2. **Evaluation Metrics**:\r\n",
    "   - **Purity**: Measures the homogeneity of clusters regarding class membership.\r\n",
    "   - **Silhouette Score**: Reflects the separation and compactness of clusters.\r\n",
    "   - **Random Index**: Measures the similarity between predicted and actual clusterings.\r\n",
    "\r\n",
    "3. **Functions**:\r\n",
    "   - `compute_purity(true_labels, cluster_labels)`: Computes the purity score based on true class labels and cluster labels.\r\n",
    "   - `compute_random_index(true_labels, cluster_labels)`: Computes the random index based on true class labels and cluster labels.\r\n",
    "\r\n",
    "4. **Usage**:\r\n",
    "   - Ensure the input data (`X_selected`) and true class labels (`true_labels`) are properly defined.\r\n",
    "   - Execute the code to obtainEADME with additional details or instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "58c9454c-07ad-4c6b-a6c3-50f812992c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zain Abbas\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "k = 5 \n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(X_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "bfe74f76-2573-4236-ae14-6fefcf5418ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_purity(true_labels, cluster_labels):\n",
    "    cm = contingency_matrix(true_labels, cluster_labels)\n",
    "    purity = np.sum(np.amax(cm, axis=0)) / np.sum(cm)\n",
    "    return purity\n",
    "\n",
    "silhouette = silhouette_score(X, cluster_labels)\n",
    "\n",
    "def compute_random_index(true_labels, cluster_labels):\n",
    "    cm = contingency_matrix(true_labels, cluster_labels)\n",
    "    a = np.sum(np.square(cm))\n",
    "    b = np.sum(np.square(np.sum(cm, axis=0))) - a\n",
    "    c = np.sum(np.square(np.sum(cm, axis=1))) - a\n",
    "    d = np.sum(np.square(np.sum(cm)) - (a + b + c))\n",
    "    rand_index = (a + d) / (a + b + c + d)\n",
    "    return rand_index\n",
    "\n",
    "true_labels = weight['classes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0a65c-0900-4760-857e-53388a7b3aea",
   "metadata": {},
   "source": [
    "## Clustering Evaluation on Limited Data\n",
    "\n",
    "This README outlines the evaluation of clustering performance using a limited dataset. Despite the constraints of limited data, the clustering algorithm demonstrates promising results.\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "1. **Purity (0.55)**:\n",
    "   - Purity measures the homogeneity of clusters regarding class membership.\n",
    "   - A score of 0.55 indicates moderate purity, suggesting some mixing of classes within clusters.\n",
    "\n",
    "2. **Silhouette Score (0.6806)**:\n",
    "   - Reflects the separation and compactness of clusters.\n",
    "   - A value of 0.6806 indicates well-separated and compact clusters, despite the limited data.\n",
    "\n",
    "3. **Random Index (0.625)**:\n",
    "   - Measures the similarity between predicted and actual clusterings.\n",
    "   - A score of 0.625 signifies significant agreement between the predicted and actual clusters.\n",
    "\n",
    "### Overall Assessment:\n",
    "\n",
    "- The clustering algorithm demonstrates promising performance given the limited data.\n",
    "- Despite moderate purity, the Silhouette Score and Random Index indicate well-separated clusters with substantial agreement between predicted and actual clusters.\n",
    "- These results suggest the algorithm's ability to effectively identify patterns and group data points, even with a smaller dataset.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Further data collection or augmentation could potentially improve cluster homogeneity (purity).\n",
    "- Experimentation with different clustering algorithms or parameter tuning may enhance performance further.\n",
    "\n",
    "### Dependencies:\n",
    "\n",
    "- Data (limited dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "763ecd0b-741d-42a0-bcd1-c935b1892e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.55\n",
      "Silhouette Score: 0.6806129558015834\n",
      "Random Index: 0.625\n"
     ]
    }
   ],
   "source": [
    "print(\"Purity:\", compute_purity(true_labels, cluster_labels))\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "print(\"Random Index:\", compute_random_index(true_labels, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d636db03-9592-4793-83e9-781704b806b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d129cdf-be13-487c-ace3-52c918f0b9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "9e502bc9-0926-4dba-a08b-a8d67cfe3c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_word = pd.read_csv('weight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "9dfc49f8-7f48-43d6-ab64-448659e1641a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 16661)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2238ad59-5a5a-40d1-b4d4-cfb119bb1fde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08653ba1-2222-4ea4-8b5b-30672d182993",
   "metadata": {},
   "source": [
    "# ADHOC PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4566b0-12f7-4429-a7a9-a00d68692af2",
   "metadata": {},
   "source": [
    "## TEST DOC ~ Cleaning and Preprocessing Pipeline\n",
    "\n",
    "This pipeline processes text data for natural language processing tasks. It includes several functions to clean and preprocess documents, including tokenization, removing conjunctions, word segmentation, and stemming.\n",
    "\n",
    "### Functions:\n",
    "\n",
    "1. **cleaning_pipeline(token_)**: Cleans tokens by removing punctuation, splitting words, and categorizing numbers separately.\n",
    "   \n",
    "2. **token_seperator(token_)**: Separates tokens into individual words using an inferred spaces approach.\n",
    "\n",
    "3. **infer_spaces(s_, maxword_, wordcost_)**: Infers spaces between words in a tokenized string.\n",
    "\n",
    "4. **remove_conjunctions_from_sets(input_list_)**: Removes conjunctions from sets of words.\n",
    "\n",
    "5. **word_segment(sentences_)**: Segments sentences into words, handling cases of len(3) words and stopwords.\n",
    "\n",
    "6. **preprocess_document(new_document_)**: Integrates all cleaning and preprocessing steps to process a document. Returns a list of stemmed terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3adcac29-599b-4c60-9eee-032159687fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_pipeline_(token_):\n",
    "    tokens_no_further_processing_required_ = []\n",
    "    token_contain_only_numbers_ = []\n",
    "    \n",
    "    for x_ in token_:\n",
    "        if len(x_) >= 4 and not x_.isdigit() and any(c_.isalpha() for c_ in x_):\n",
    "            new_x_ = ''.join(' ' if char_ in punctuation else char_ for char_ in x_)\n",
    "            if ' ' in new_x_:\n",
    "                token_split_ = new_x_.split(' ')\n",
    "                for word_ in token_split_:\n",
    "                    if len(word_) >= 4:\n",
    "                        tokens_no_further_processing_required_.append(word_)\n",
    "            else:\n",
    "                if len(new_x_) >= 4:\n",
    "                    tokens_no_further_processing_required_.append(new_x_)\n",
    "        elif x_.isdigit():\n",
    "            token_contain_only_numbers_.append(x)\n",
    "    \n",
    "    return tokens_no_further_processing_required_, [], token_contain_only_numbers_\n",
    "\n",
    "\n",
    "\n",
    "def token_seperator_(token_):\n",
    "    words_ = list(english_words)\n",
    "\n",
    "    wordcost_ = {k: log((i + 1) * log(len(words_))) for i, k in enumerate(words_)}\n",
    "    maxword_ = max(len(x_) for x_ in words_)\n",
    "    clean_term_ = []\n",
    "    for i_ in range(len(token_)):\n",
    "        clean_term_.append((infer_spaces(token_[i_][0].lower(), maxword_, wordcost_)))\n",
    "    return cleaning_pipeline(clean_term_)\n",
    "    \n",
    "\n",
    "def infer_spaces_(s_, maxword_, wordcost_):\n",
    "    def best_match(i_):\n",
    "        candidates = enumerate(reversed(cost[max(0, i_ - maxword_):i_]))\n",
    "        return min((c + wordcost_.get(s_[i_ - k_ - 1:i_], 9e999), k_ + 1) for k_, c in candidates)\n",
    "    \n",
    "    cost = [0]\n",
    "    for i_ in range(1, len(s_) + 1):\n",
    "        c, k_ = best_match(i_)\n",
    "        cost.append(c)\n",
    "    \n",
    "    out = []\n",
    "    i_ = len(s_)\n",
    "    while i_ > 0:\n",
    "        c, k_ = best_match(i_)\n",
    "        assert c == cost[i_]\n",
    "        out.append(s_[i_ - k_:i_])\n",
    "        i_ -= k_\n",
    "    \n",
    "    return \" \".join(reversed(out))\n",
    "    \n",
    "#######################################\n",
    "\n",
    "def remove_conjunctions_from_sets_(input_list_):\n",
    "    conjunctions_ = ['this', 'that','of', 'and', 'or', 'but', 'for', 'nor', 'so', 'yet', 'to', 'with', 'in', 'on', 'at', 'by', 'is', \"the\", \"of\", \"all\", \"and\", \"to\", \"can\", \"be\", \"as\", \"once\", \"for\", \"at\", \"am\", \"are\", \"has\", \"have\", \"had\", \"up\", \"his\", \"her\", \"in\", \"on\", \"no\"]\n",
    "    filtered_list_ = []\n",
    "    for s_ in input_list_:  # Unpack the tuple\n",
    "        # Check from the beginning\n",
    "        for i_ in range(len(s_)):\n",
    "            first_word_ = s_[:i_].lower()\n",
    "            if first_word_ in conjunctions_:\n",
    "                s_ = s_[i_:]\n",
    "                break\n",
    "        # Check from the end\n",
    "        for i_ in range(len(s_), 0, -1):\n",
    "            last_word_ = s_[i_:].lower()\n",
    "            if last_word_ in conjunctions_:\n",
    "                s_ = s_[:i_]\n",
    "                break\n",
    "        filtered_list_.append(s_)  # Append the modified tuple\n",
    "    return filtered_list_\n",
    "\n",
    "\n",
    "def word_segment_(sentences_):\n",
    "    len_3_word_ = []\n",
    "    segmented_sentences_ = []\n",
    "    term_ = []\n",
    "    for sentence_ in sentences_:\n",
    "        words_ = []\n",
    "        start_ = 0\n",
    "        while start_ < len(sentence_):\n",
    "            found_ = False\n",
    "            for end_ in range(len(sentence_), start_, -1):\n",
    "                word_ = sentence_[start_:end_]\n",
    "                if word_.lower() in english_words:\n",
    "                    if word_.lower().startswith(\"the\"):\n",
    "                        word_ = word_[2:]\n",
    "                    if len(word_) >= 4:\n",
    "                        if word_.lower() not in {\"a\", \"an\", \"the\"} or start_ != 0:\n",
    "                            words_.append(word_)\n",
    "                        start_ = end_\n",
    "                        found_ = True\n",
    "                    else:\n",
    "                        len_3_word_.append(word_)\n",
    "                    break\n",
    "            if not found_:\n",
    "                start_ += 1\n",
    "        segmented_sentences_.append(words_)\n",
    "\n",
    "    term_ = [wd.lower() for wd_ in len_3_word_ if len(wd_) >= 3 and wordnet.synsets(word_, pos=wordnet.NOUN) and word_.lower() not in stop_words and word_ not in punctuation]\n",
    "    for x_ in segmented_sentences_:\n",
    "        for word_ in x_:\n",
    "            if word_ not in stop_words and word_ in english_words:\n",
    "                term_.append(word_)\n",
    "    return term_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "5b99a137-1e6c-4c87-bf4f-aad5a9cb2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "\n",
    "# Initialize Porter Stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def preprocess_document(new_document_):\n",
    "    \n",
    "    tokens_ = []\n",
    "    token_exist_in_vocab_ = []\n",
    "    tokens_no_further_processing_required_ = []\n",
    "    clean_token_ = []\n",
    "    token_contain_only_numbers_ = []\n",
    "    tokens_no_further_processing_required1_ = []\n",
    "    clean_token1_ = []\n",
    "    token_contain_only_numbers1_ = []\n",
    "    tokens_no_further_processing_required2_ = []\n",
    "    clean_token2_ = []\n",
    "    token_contain_only_numbers2_ = []\n",
    "    term_ = []\n",
    "    stem_terms_ = []\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens_ = re.findall(r'\\b\\w+\\b', new_document_.lower())\n",
    "    \n",
    "    # Cleaning pipeline\n",
    "    token_have_hyphen_ = [x for x in tokens_]\n",
    "    token_link_ = [x for x in token_have_hyphen_ if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "    token_have_hyphen_ = [x for x in token_have_hyphen_ if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "    hyphen_term_ = [x for x in token_have_hyphen_ if x.count('-') == 1 and '-' not in [x[0], x[-1]] and 4 <= len(x.split('-')[0]) + len(x.split('-')[1]) <= 20]\n",
    "    token_have_hyphen_ = [x for x in token_have_hyphen_ if (x,) not in hyphen_term_]\n",
    "\n",
    "    token_link_ = [x for x in token_have_hyphen_ if x.startswith('//') or x.startswith('www') or x.startswith('org')]\n",
    "    token_have_hyphen_ = [x for x in token_have_hyphen_ if not (x.startswith('//') or x.startswith('www') or x.startswith('org'))]\n",
    "\n",
    "    token_exist_in_vocab_ = [x for x in token_have_hyphen_ if x in words.words()]\n",
    "    token_have_hyphen_ = [x for x in token_have_hyphen_ if x not in words.words()]\n",
    "\n",
    "    tokens_no_further_processing_required_, clean_token_, token_contain_only_numbers_ = cleaning_pipeline_(token_have_hyphen_)\n",
    "    all_number_ = token_contain_only_numbers_\n",
    "    tokens_no_further_processing_required1_, clean_token1_, token_contain_only_numbers1_ = cleaning_pipeline_(hyphen_term_)\n",
    "    all_number_ += token_contain_only_numbers1_\n",
    "\n",
    "    # Token separation and further processing\n",
    "    tokens_no_further_processing_required2_, clean_token2_, token_contain_only_numbers2_ = token_seperator_(clean_token_)\n",
    "    term_ = token_exist_in_vocab_ + tokens_no_further_processing_required_ + tokens_no_further_processing_required1_ + tokens_no_further_processing_required2_\n",
    "    \n",
    "    # Stemming\n",
    "    stem_terms_ = []\n",
    "    for x in term_:\n",
    "        stemmed_x = porter.stem(x)\n",
    "        stem_terms_.append(stemmed_x)\n",
    "    \n",
    "    return stem_terms_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cccb3d-7f42-4002-8756-9a0237f23e39",
   "metadata": {},
   "source": [
    "\r\n",
    "\r\n",
    "## Text Classification using TF-IDF and KNN\r\n",
    "\r\n",
    "This script classifies text documents into predefined categories using TF-IDF (Term Frequency-Inverse Document Frequency) feature extraction and K-Nearest Neighbors (KNN) classification.\r\n",
    "\r\n",
    "### Steps:\r\n",
    "\r\n",
    "1. **Preprocessing**:\r\n",
    "   - The `preprocess_document` function tokenizes and preprocesses the input text document, producing a list of stemmed terms.\r\n",
    "   \r\n",
    "2. **TF-IDF Transformation**:\r\n",
    "   - The script calculates the TF-IDF values for each term in the document using a pre-trained TF-IDF weight matrix (`weight_word`).\r\n",
    "   \r\n",
    "3. **KNN Classification**:\r\n",
    "   - For each term in the document, if it exists in the TF-IDF weight matrix, the corresponding TF-IDF value is retrieved. Otherwise, a default value of 0 is assigned.\r\n",
    "   - The TF-IDF values are padded with zeros to match the required input shape for classification.\r\n",
    "   - The KNN classifier (`knn`) predicts the category of the document based onional details or instructions as needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "aee8b192-85ee-4152-8753-4d6d6f6d1dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: [3]\n"
     ]
    }
   ],
   "source": [
    "new_document = ''' explainability in Artificial Intelligence (AI) has been revived as a topic ofactive research by the need of conveying safety and trust to users in the\n",
    "“how”and “why”of automated decision-making in different applications '''\n",
    "\n",
    "tokens = preprocess_document(new_document)\n",
    "\n",
    "tfidf_values = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token in weight_word.columns:\n",
    "        tfidf_value = weight_word[token].iloc[0] \n",
    "        tfidf_values.append(tfidf_value)\n",
    "    else:\n",
    "        tfidf_values.append(0)  \n",
    "\n",
    "tfidf_values += [0] * (1000 - len(tfidf_values))\n",
    "tfidf_array = np.array(tfidf_values).reshape(1, -1) \n",
    "predicted_category = knn.predict(tfidf_array)\n",
    "\n",
    "print(\"Predicted category:\", predicted_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f210e999-832d-4f61-bc66-fb11a2a21076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0195d-b136-458f-8b1e-6c5b47480ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
